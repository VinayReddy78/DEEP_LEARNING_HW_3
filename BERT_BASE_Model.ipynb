{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5368ded9-31a9-4865-b433-58b0b06c37c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-31 23:39:55.118127: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-31 23:39:55.243164: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-31 23:39:56.128767: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib\n",
      "2023-03-31 23:39:56.128823: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib\n",
      "2023-03-31 23:39:56.128827: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel, BertTokenizerFast, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c211c4d-7a5b-4695-9572-bb459549923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path): \n",
    "    \n",
    "    with open(path, 'rb') as f:\n",
    "        raw_data = json.load(f)\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    num_q = 0\n",
    "    num_pos = 0\n",
    "    num_imp = 0\n",
    "    for group in raw_data['data']:\n",
    "        for paragraph in group['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                question       = qa['question']\n",
    "                num_q  = num_q+1\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context.lower())\n",
    "                    questions.append(question.lower())\n",
    "                    answers.append(answer)\n",
    "    return num_q, num_pos, num_imp, contexts, questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcd39b4a-f8cf-4893-8587-be5441ec26cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_questions = 0\n",
    "num_of_possible = 0\n",
    "num_of_impossible = 0\n",
    "num_q, num_pos, num_imp, train_contexts, train_questions, train_answers = get_data('spoken_train-v1.1.json')\n",
    "num_of_questions  = num_q\n",
    "num_of_possible = num_pos\n",
    "num_of_impossible  = num_imp\n",
    "\n",
    "num_q, num_pos, num_imp, valid_contexts, valid_questions, valid_answers = get_data('spoken_test-v1.1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44dc7fae-7b57-404f-8cfe-a2bf9ec52243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_answer_at_end(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        answer['text'] = answer['text'].lower()\n",
    "        answer['answer_end'] = answer['answer_start'] + len(answer['text'])\n",
    "\n",
    "add_answer_at_end(train_answers, train_contexts)\n",
    "add_answer_at_end(valid_answers, valid_contexts)\n",
    "\n",
    "\n",
    "MAX_LENGTH = 250\n",
    "MODEL_PATH = \"bert-base-uncased\"\n",
    "\n",
    "tokenizerFast = BertTokenizerFast.from_pretrained(MODEL_PATH)\n",
    "train_encodings_fast = tokenizerFast(train_questions, train_contexts,  max_length = MAX_LENGTH, truncation=True, padding=True)\n",
    "valid_encodings_fast = tokenizerFast(valid_questions,valid_contexts,  max_length = MAX_LENGTH, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a83db793-e8b0-40a2-bcb2-a8bdf484090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_Answer_startandend_train(idx):\n",
    "    return_start = 0\n",
    "    return_end = 0\n",
    "    answer_encoding_fast = tokenizerFast(train_answers[idx]['text'],  max_length = MAX_LENGTH, truncation=True, padding=True)\n",
    "    for a in range( len(train_encodings_fast['input_ids'][idx]) -  len(answer_encoding_fast['input_ids']) ): \n",
    "        match = True\n",
    "        for i in range(1,len(answer_encoding_fast['input_ids']) - 1):\n",
    "            if (answer_encoding_fast['input_ids'][i] != train_encodings_fast['input_ids'][idx][a + i]):\n",
    "                match = False\n",
    "                break\n",
    "            if match:\n",
    "                return_start = a+1\n",
    "                return_end = a+i+1\n",
    "                break\n",
    "    return(return_start, return_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dae5db2c-866d-4471-aaaf-dbc6eaffd41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478\n"
     ]
    }
   ],
   "source": [
    "start_positions = []\n",
    "end_positions = []\n",
    "counter = 0\n",
    "for t in range(len(train_encodings_fast['input_ids'])):\n",
    "    s,e = return_Answer_startandend_train(t)\n",
    "    start_positions.append(s)\n",
    "    end_positions.append(e)\n",
    "    if s==0:\n",
    "        counter = counter + 1\n",
    "train_encodings_fast.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "print(counter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b900aae-0d57-4a4f-be5f-346406ea1b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_answer_startend_valid(idx):\n",
    "    return_start = 0\n",
    "    return_end = 0\n",
    "    answer_encoding_fast = tokenizerFast(valid_answers[idx]['text'],  max_length = MAX_LENGTH, truncation=True, padding=True)\n",
    "    for a in range( len(valid_encodings_fast['input_ids'][idx])  -  len(answer_encoding_fast['input_ids'])   ): \n",
    "        match = True\n",
    "        for i in range(1,len(answer_encoding_fast['input_ids']) - 1):\n",
    "            if (answer_encoding_fast['input_ids'][i] != valid_encodings_fast['input_ids'][idx][a + i]):\n",
    "                match = False\n",
    "                break\n",
    "            if match:\n",
    "                return_start = a+1\n",
    "                return_end = a+i+1\n",
    "                break\n",
    "    return(return_start, return_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db2fbbe8-f5d3-4f14-b2b3-90f28727bd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236\n"
     ]
    }
   ],
   "source": [
    "start_positions = []\n",
    "end_positions = []\n",
    "counter = 0\n",
    "for h in range(len(valid_encodings_fast['input_ids']) ):\n",
    "   \n",
    "    s, e = return_answer_startend_valid(h)\n",
    "    start_positions.append(s)\n",
    "    end_positions.append(e)\n",
    "    if s==0:\n",
    "        counter = counter + 1\n",
    "\n",
    "valid_encodings_fast.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "print(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1ad2c4b-9f0d-4226-a0d2-aafcfc0b3fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class InputDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.encodings['input_ids'][i]),\n",
    "            'token_type_ids': torch.tensor(self.encodings['token_type_ids'][i]),\n",
    "            'attention_mask': torch.tensor(self.encodings['attention_mask'][i]),\n",
    "            'start_positions': torch.tensor(self.encodings['start_positions'][i]),\n",
    "            'end_positions': torch.tensor(self.encodings['end_positions'][i])\n",
    "        }\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "train_dataset = InputDataset(train_encodings_fast)\n",
    "valid_dataset = InputDataset(valid_encodings_fast)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=1)\n",
    "\n",
    "bert_model = BertModel.from_pretrained(MODEL_PATH)  #MODEL_PATH = \"bert-base-uncased\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c907021-0f8d-4266-9bda-92b901fdc313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QAModel, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "        self.l1 = nn.Linear(768 * 2, 768 * 2)\n",
    "        self.l2 = nn.Linear(768 * 2, 2)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            self.drop_out,\n",
    "            self.l1,\n",
    "            nn.LeakyReLU(),\n",
    "            self.l2 \n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        model_output = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=True)\n",
    "        hidden_states = model_output[2]\n",
    "        out = torch.cat((hidden_states[-1], hidden_states[-3]), dim=-1)  \n",
    "        logits = self.linear_relu_stack(out)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        return start_logits, end_logits\n",
    "\n",
    "model = QAModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3d12a92-4cf1-4f21-8ed2-b330787a44c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smunde/.local/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def loss_function(start_logits, end_logits, start_positions, end_positions):\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    start_loss = loss_fct(start_logits, start_positions)\n",
    "    end_loss = loss_fct(end_logits, end_positions)\n",
    "    total_loss = (start_loss + end_loss)/2\n",
    "    return total_loss\n",
    "\n",
    "def focal_loss_function(start_logits, end_logits, start_positions, end_positions, gamma):\n",
    "    smax = nn.Softmax(dim=1)\n",
    "    probs_start = smax(start_logits)\n",
    "    inv_probs_start = 1 - probs_start\n",
    "    probs_end = smax(end_logits)\n",
    "    inv_probs_end = 1 - probs_end\n",
    "    lsmax = nn.LogSoftmax(dim=1)\n",
    "    log_probs_start = lsmax(start_logits)\n",
    "    log_probs_end = lsmax(end_logits)\n",
    "    \n",
    "    nll = nn.NLLLoss()\n",
    "    \n",
    "    fl_start = nll(torch.pow(inv_probs_start, gamma)* log_probs_start, start_positions)\n",
    "    fl_end = nll(torch.pow(inv_probs_end, gamma)*log_probs_end, end_positions)\n",
    "    return ((fl_start + fl_end)/2)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=2e-5, weight_decay=2e-2)\n",
    "scheduler = ExponentialLR(optim, gamma=0.9)\n",
    "total_acc = []\n",
    "total_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ada05c7b-7c07-49bd-a52d-a31fa2a27e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, epoch):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    acc = []\n",
    "    counter = 0\n",
    "    batch_tracker = 0\n",
    "    for batch in tqdm(dataloader, desc = 'Running Epoch '):\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        out_start, out_end = model(input_ids=input_ids, \n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids)\n",
    "        \n",
    "        loss = focal_loss_function(out_start, out_end, start_positions, end_positions,1) \n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        start_predictor = torch.argmax(out_start, dim=1)\n",
    "        end_pred = torch.argmax(out_end, dim=1)\n",
    "            \n",
    "        acc.append(((start_predictor == start_positions).sum()/len(start_predictor)).item())\n",
    "        acc.append(((end_pred == end_positions).sum()/len(end_pred)).item())\n",
    "       \n",
    "        batch_tracker = batch_tracker + 1\n",
    "        if batch_tracker==250 and epoch==1:\n",
    "            total_acc.append(sum(acc)/len(acc))\n",
    "            loss_avg = sum(losses)/len(losses)\n",
    "            total_loss.append(loss_avg)\n",
    "            batch_tracker = 0\n",
    "    scheduler.step()\n",
    "    ret_acc = sum(acc)/len(acc)\n",
    "    ret_loss = sum(losses)/len(losses)\n",
    "    return(ret_acc, ret_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14d23dcc-e65d-4b7a-ad9d-5789d3bca845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    acc = []\n",
    "    counter = 0\n",
    "    answer_list=[]\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc = 'Running the Evaluation'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            start_true = batch['start_positions'].to(device)\n",
    "            end_true = batch['end_positions'].to(device)\n",
    "            \n",
    "            out_start, out_end = model(input_ids=input_ids, attention_mask=attention_mask,token_type_ids=token_type_ids)\n",
    "            \n",
    "            start_predictor = torch.argmax(out_start)\n",
    "            end_pred = torch.argmax(out_end)\n",
    "            answer = tokenizerFast.convert_tokens_to_string(tokenizerFast.convert_ids_to_tokens(input_ids[0][start_predictor:end_pred]))\n",
    "            tanswer = tokenizerFast.convert_tokens_to_string(tokenizerFast.convert_ids_to_tokens(input_ids[0][start_true[0]:end_true[0]]))\n",
    "            answer_list.append([answer,tanswer])\n",
    "        \n",
    "    return answer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8ffe74a-412d-484d-8696-367609dd4f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 3.3 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from evaluate) (2022.2.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/smunde/.local/lib/python3.9/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from evaluate) (2.27.1)\n",
      "Requirement already satisfied: xxhash in /home/smunde/.local/lib/python3.9/site-packages (from evaluate) (3.2.0)\n",
      "Requirement already satisfied: packaging in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: multiprocess in /home/smunde/.local/lib/python3.9/site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/smunde/.local/lib/python3.9/site-packages (from evaluate) (2.10.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/smunde/.local/lib/python3.9/site-packages (from evaluate) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from evaluate) (4.64.0)\n",
      "Requirement already satisfied: pandas in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from evaluate) (1.4.2)\n",
      "Requirement already satisfied: dill in /home/smunde/.local/lib/python3.9/site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from evaluate) (1.21.5)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/smunde/.local/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\n",
      "Requirement already satisfied: aiohttp in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.1.1)\n",
      "Requirement already satisfied: filelock in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from packaging->evaluate) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2021.10.8)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from pandas->evaluate) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8156e86f-61a9-46b3-a3d1-6ea501bd92b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Epoch : 100%|██████████| 2320/2320 [09:04<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.41684113301593684      Train Loss: 2.143070732930611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running the Evaluation: 100%|██████████| 15875/15875 [02:29<00:00, 106.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.005165354330709]\n",
      "['this', 'is', 'a', 'sentence', '.']\n",
      "this is a sentence.\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "wer = load(\"wer\")\n",
    "EPOCHS = 1\n",
    "model.to(device)\n",
    "wer_list=[]\n",
    "for epoch in range(EPOCHS):\n",
    "    train_acc, train_loss = train_epoch(model, train_data_loader, epoch+1)\n",
    "    print(f\"Train Accuracy: {train_acc}      Train Loss: {train_loss}\")\n",
    "    answer_list = evaluate_model(model, valid_data_loader)\n",
    "    pred_answers=[]\n",
    "    true_answers=[]\n",
    "    for i in range(len(answer_list)):\n",
    "        if(len(answer_list[i][0])==0):\n",
    "            answer_list[i][0]=\"$\"\n",
    "        if(len(answer_list[i][1])==0):\n",
    "            answer_list[i][1]=\"$\"\n",
    "        pred_answers.append(answer_list[i][0])\n",
    "        true_answers.append(answer_list[i][1])\n",
    "    wer_score = wer.compute(predictions=pred_answers, references=true_answers)\n",
    "    wer_list.append(wer_score)\n",
    "print(wer_list)\n",
    "\n",
    "tokens = tokenizerFast.tokenize(\"This is a sentence.\")\n",
    "print(tokens)\n",
    "output = tokenizerFast.convert_tokens_to_string(tokens)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3008daba-4c3c-4c64-bfdf-67f5045d5db9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
